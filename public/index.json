
[{"content":"gibby /g…™b.i:/ .n 1 computer person 2 hobby haver\n","date":"21 March 2025","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"21 March 2025","externalUrl":null,"permalink":"/series/pedal-steel/","section":"Series","summary":"","title":"Pedal Steel","type":"series"},{"content":"","date":"21 March 2025","externalUrl":null,"permalink":"/tags/pedal-steel/","section":"Tags","summary":"","title":"Pedal Steel","type":"tags"},{"content":"","date":"21 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"21 March 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"21 March 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" An introduction to an enigmatic instrument The Pacific Northwest is home to a bunch of interesting antique stores. A couple of years ago, I was browsing musical antiques at a shop in Bellingham, WA. Among the vintage records, accordions, and old sound systems, my attention was drawn to a stack of flat, ornate looking stringed instruments. Turns out, I had come upon a pile of zithers.\nWhat is a zither? # A zither is a stringed instrument where strings are stretched over a thin, flat body without extending onto a separate neck or fingerboard. Think of an acoustic guitar or a cello - these instruments are not zithers because their strings span the entire body of the instrument, past the sounding box and onto necks. (Plus, neither instrument is thin, nor flat.)\nThere are several categories of zithers; I think tube zithers are particularly cool. To most, the prototypical zither looks and sounds like this:\nTo my untrained eye, a zither looks like a tiny harp with several strings (some clustered together) and a fretboard. I think you\u0026rsquo;d call this a concert zither, which is a type of zither popular in Eastern Europe. I didn\u0026rsquo;t find any concert zithers in Bellingham though; I found chord zithers, which are more common in North America.\nChord zithers # As the name implies, chord zithers make it easier to play chords. To me, a chord zither is distinguished by some special set of strings (or some special set of buttons) that are used to play chords instead of notes.\nThe autoharp is a relatively well-known chord zither. Fans of 60s pop might remember that Do You Believe in Magic by The Lovin\u0026rsquo; Spoonful features the autoharp:\nHidden beneath the autoharp\u0026rsquo;s \u0026ldquo;plate\u0026rdquo; of buttons, there are several bars affixed with soft pads. When an autoharp\u0026rsquo;s chord button is pressed, the bar is lowered onto the strings and pads will mute all strings except for the strings needed to produce a given chord.\nNotice the white pads on the underside of the frontmost bar. Photo from Heights Guitar Tech. In the 80s, Suzuki released the Omnichord, which is basically an electronic/synthesized version of an autoharp. Omnichords offer the same \u0026ldquo;button chording\u0026rdquo; functionality as the autoharp, but the two instruments sound very different:\nI\u0026rsquo;ve been looking for a new instrument to learn, so I tried to learn more about chord zithers when I got home from antiquing. As I watched various Youtube videos and scraped the autoharp subreddit, I could see myself playing around with these instruments, but I wasn\u0026rsquo;t quite sold yet. I was determined to find my zither soulmate.\nAs I continued my exploration of the zither family, I eventually encountered the tremoloa.\nTremoloa # The tremoloa is a really, really weird zither.\nThe body of the tremoloa is dominated by a steel arm which supports a weighted rolling bar. The \u0026ldquo;melody string\u0026rdquo; running along the bottom of the instrument can be plucked using a plectrum at the end of the arm. When the melody string is plucked, the string vibrates against the weighted bar, which produces a janky, ghoulish sound. Acommpanying notes can be played with the left hand using the strings that run along the top of the instrument.\nIf the autoharp is niche, then the tremoloa is on another plane of niche existence. This is a fretless instrument - those markings near the melody string may seem fret-like, but from what I understand, they\u0026rsquo;re not very helpful when playing. I was disappointed to learn that tremoloa is so difficult to learn, because I was getting soulmate vibes from this barred stringed zither.\nIn my research, I found that the tremoloa was created as an attempt to market Hawaiian steel guitar to American consumers.\nWhat is steel guitar? # I\u0026rsquo;ve always found the phrase \u0026lsquo;steel guitar\u0026rsquo; a bit confusing. Having played classical guitar - which use soft nylon strings instead of steel strings - I used to wonder, is a \u0026lsquo;steel guitar\u0026rsquo; just\u0026hellip; a regular guitar?\nSteel guitar specifically refers to guitar that\u0026rsquo;s played with a bar (or some other object) pressed against its strings.\nIf you\u0026rsquo;re familiar with blues, you might be familiar with the picture of someone playing a standard acoustic guitar with a metal or glass tube on a fretting finger finger. The tube is slid across the fretboard, producing a distinctly twangy, resonant tone when the strings are plucked with the other hand. Folks also refer to this playing style as \u0026ldquo;slide guitar\u0026rdquo;.\nSlide guitar is typically played upright - the guitar body held against the player\u0026rsquo;s body with strings facing outward - much like the playing position you\u0026rsquo;d use with standard electric or acoustic guitar.\nThis technique likely originated in Hawaii, where it became known as Hawaiian steel guitar. Unlike upright slide guitar, Hawaiian steel guitar is typically played with the guitar laid horizonally across the musician\u0026rsquo;s lap. One hand glides a tone bar along the fretboard to alter pitch, while the other hand picks or strums the strings. Hawaiian steel guitar is associated with C6 tuning - also known as open C (C-E-G-A-C-E) - which contributes to a unique melodic quality that sets it apart from blues-style steel guitar.\nI\u0026rsquo;m grateful for Hawaiian steel guitar because it helps me provide others with a point of reference when I\u0026rsquo;m talking about pedal steel. People my age aren\u0026rsquo;t always familiar with pedal steel, but most of them have seen SpongeBob.\nHawaiian steel guitar made its way to the continental United States in the early 1900s and became wildly popular. As I mentioned before, blues and rock adopted the upright style of slide guitar. Classic lap style slide guitar became widely known in various genres as lap steel guitar.\nLap steel guitar # Lap steel guitars look like electric guitars with bodies trimmed down to lap size.\nA vintage Guyatone lap steel. Guyatone (a Japanese company) would have sold this guitar in the 1960s. Photo from Gravity Music Gear. Lap steels are six-stringed instruments, and C6 is still a popular tuning for lap steel. Of course, other tunings are possible - the key challenge of lap steel lies in selecting the best tuning and developing the technique to achieve the harmonies, smooth transitions, and expression that best suit a particular song.\nIf an autoharp is the pedal steel‚Äôs cousin, then the lap steel is its younger brother - similar in spirit, but simpler and more approachable. Many beginner-level pedal steel songs are playable on lap steel, which makes it a great entry point into this style of playing. They‚Äôre also affordable, with excellent used models available in the $200 range. Lap steel probably deserves its own dedicated series; I haven‚Äôt sourced one yet, but it‚Äôs definitely on my list.\nNote\nWhen I was deciding on my entry point into steel guitar, I found that a lot of folks argue, \u0026ldquo;If you want to learn pedal steel, learn pedal steel. If you want to learn lap steel, learn lap steel. The basic technique is similar, but the instruments are different enough that most essential skills are not transferrable.\u0026rdquo;\nInitially this argument persuaded me, but my perspective has changed after getting my own pedal steel. Pedal steel guitars are expensive, and if you\u0026rsquo;ve never played an instrument like this (or if you\u0026rsquo;ve never played an instrument at all), you\u0026rsquo;re taking a leap of faith.\nFor ~$100, you can buy a used lap steel and learn the basics of picking, barring, and theory. If you enjoy learning and playing lap steel, you\u0026rsquo;ll have a better idea of whether the pedal steel plunge makes sense for you.\nAs lap steel guitars grew in popularity, enhancements were introduced to improve their ergonomics and functionality. One notable advancement was the addition of palm levers (also known as benders).\nA Peters Classic palm lever lap steel. Photo from Peters Instruments. Today, you can buy these palm levers separately and install them on most lap steel models. When activated, each lever pulls its corresponding string, instantly raising or lowering its pitch. This quick shift in pitch produces a distinctive and expressive sound. In this video, notice how the notes change as each palm lever is pressed:\nConsole steel guitar # Eventually, lap steels were produced with multiple necks - usually two, but nutjob guitars like this Fender Quad Stringmaster featured four necks.\nA Fender Quad Stringmaster. Photo from The Steel Guitar Forum. The idea behind multiple necks was to let players quickly alternate between tunings without retuning between songs. However, these necks added weight and size, so the guitars were often mounted on four-legged stands and called console steel guitars. Musicians play console steel either seated or standing behind the console.\nAside from adding necks, console steel guitars also added strings. Many came equipped with eight strings instead of the usual six.\nInfo\nOn a two neck console steel guitar, it\u0026rsquo;s common for one neck to be tuned to C6 and the other neck tuned to E9 (E-G‚ôØ-B-D-F‚ôØ-G‚ôØ-B-E). This eight string E9 tuning is associated with Western swing, a delightful genre that combines elements of country, blues, and jazz. Pedal steel has its own E9 tuning, also known as Nashville E9 (B-D‚ÄìE‚ÄìF‚ôØ‚ÄìG‚ôØ‚ÄìB‚ÄìE‚ÄìG‚ôØ‚ÄìD‚ôØ‚ÄìF‚ôØ).\nIn my eyes, Santo \u0026amp; Johnny‚Äôs Sleep Walk will always be the most iconic steel guitar song ever recorded. In this recording, Santo plays on a three-neck Fender Stringmaster console steel guitar:\nAccording to folks on the Steel Guitar Forum, the liner notes of Legends Of Guitar: Rock, 1950s, Vol. 2 quote Santo as saying: \u0026ldquo;I [was tuned] to C#m7 on one neck\u0026hellip; A6 on another, and E7 on the third neck. Basically, I could play Sleep Walk on any of the necks, but I recorded it with the C#m7.\u0026rdquo;\nWith multiple necks, palm levers, and additional strings, the steel guitar had evolved into a vibrantly expressive instrument and found its place in mainstream music. With a few more innovations, it would reach its final evolution: pedal steel guitar.\nPedal steel guitar # As its name implies, the pedal steel was born when foot (or floor) pedals were integrated into console steel guitars. The first pedal steel guitar was designed by Paul Bigsby in the late 1940s. His Bigsby two-pedal steel guitar was famously played by Speedy West, one of the most acclaimed steel guitarists of all time.\nBigsby positioned pedals on a rack between the two front legs of a console steel, connecting each pedal to a mechanical system that applied tension to the strings above. While similar in concept to lap steel palm levers, floor pedals can simultaneously alter the tension of multiple strings (like an autoharp). The internal mechanisms supporting pedals are intricate and compact, and pedal configurations can vary significantly between different pedal steel models.\nSince pedals were operated by foot, it was much easier to apply tension to a string while note was still sounding. This iconic pitch-bending sound has become the hallmark of pedal steel guitar. (Today, palm levers are marketed as enabling \u0026ldquo;pedal steel sound\u0026rdquo; in lap steel guitars.)\nInfo\nThe pedal steel guitar was developed further by Buddy Emmons in the late 1950s. He\u0026rsquo;s credited with introducing the ten-string neck (ushering in the age of Nashville E9 tuning) and refining pedal function. Emmons co-founded Sho-Bud, the first dedicated pedal steel guitar manufacturing company\nSome pedal steel guitars also have knee levers, which perform the same function as floor pedals. Knee levers hang from the underside of the pedal steel. The musician activates the knee lever by pressing against it with their knee or inner thigh.\nKnee lever close-up. Photo from The Steel Guitar Forum. Knee levers are less common than floor pedals, and some student models might not include knee levers at all. Floor pedals tend to be easier to operate, but advanced pedal steel players frequently use both pedals (often two at a time) and knee levers simultaneously to achieve intricate note adjustments.\nA classic pedal steel guitar will have three floor pedals and one knee lever. A double necked pedal steel will usually have eight floor pedals. My pedal steel guitar has nine floor pedals and eight knee levers. Since pedal steel guitars are still typically handmade or produced in limited quantities, configurations can vary widely from model to model.\nToday, pedal steel continues to find its place in radio-friendly country music. The instrument is undeniably rooted in country and folk, and many pedal steel masters remain devoted traditionalists. However, I‚Äôve always believed its voice-like sound makes it adaptable and well-suited to a wide variety of genres. In my mind, Faye Webster stands out as an artist pioneering non-traditional pedal steel use in popular music:\nUltimately, the pedal steel guitar is a complex, wonderfully expressive instrument. Too bad it\u0026rsquo;s impossible to learn!\nThe trouble with learning pedal steel # I think there are several reasons why pedal steel is inaccessible today:\nCompared to instruments like piano or clarinet, the pedal steel is an obscure instrument. Many people haven‚Äôt even heard of it - though they‚Äôve probably heard its distinctive sound without realizing it. Student models are ridiculously expensive ($1000+). A used market exists, but since all models are heavy (50+ lbs) shipping is difficult and expensive. Many used pedal steels are available for local pick-up only. Variation between models makes it difficult to provide \u0026ldquo;one sizes fits all\u0026rdquo; instruction - are you trying to learn C6 tuning without any knee levers, or E9 tuning with 12 foot pedals and 8 knee levers? Pedal steels require non-trivial maintenance. Without access to an experienced pedal steel player, new learners often struggle to handle repairs or adjustments independently. I\u0026rsquo;ve spoken with experienced pedal steel players who refer to the instrument as a \u0026ldquo;theory machine\u0026rdquo;. The complexity of pedal steel attracts musical theory nerds, which means that most pedal steel resources are dense and hard to understand if you don\u0026rsquo;t come from a musical background. Each of these factors has contributed to the pedal steel guitar\u0026rsquo;s sparse educational landscape.\nAs I\u0026rsquo;m embarking on my own journey to learn pedal steel, I\u0026rsquo;ve found that it\u0026rsquo;s absurdly difficult to answer basic, beginner-level questions about pedal steel guitar. With this series, I aim to improve the availability of pedal steel learning resources by documenting my research and learnings from a true beginner\u0026rsquo;s perspective. I\u0026rsquo;ll start with equipment and setup, then progress into guides covering theory and technique.\nPedal steel may be intimidating, but I love it and I\u0026rsquo;m eager to demystify it for myself and others. In my next post, I\u0026rsquo;ll walk through my budget-friendly(ish) beginner\u0026rsquo;s setup. See you then!\n","date":"21 March 2025","externalUrl":null,"permalink":"/posts/pedal_steel_intro/","section":"Posts","summary":"An introduction to an enigmatic instrument","title":"What is pedal steel guitar?","type":"posts"},{"content":"","date":"22 February 2025","externalUrl":null,"permalink":"/tags/beginner/","section":"Tags","summary":"","title":"Beginner","type":"tags"},{"content":"","date":"22 February 2025","externalUrl":null,"permalink":"/tags/build-systems/","section":"Tags","summary":"","title":"Build Systems","type":"tags"},{"content":"","date":"22 February 2025","externalUrl":null,"permalink":"/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"CMake is one of the most popular build systems for C/C++ today, and its flexibility makes it a great choice for C projects of any size. Many CMake tutorials exist (including this official one) but I usually find them difficult to follow; they tend to assume that the reader has pre-existing CMake experience, and they\u0026rsquo;re usually either too detailed or not detailed enough.\nHere\u0026rsquo;s my take on a practical, beginner-friendly guide to CMake.\nWhat does CMake do? # TL;DR: CMake builds your build system, which it then uses to compile and link your code.\nIDEs like Visual Studio and CLion leverage their own build systems under the hood. They allow the user to tweak their build configuration, but reasonable defaults are selected on the user\u0026rsquo;s behalf.\nCommand-line friendly C/C++ builds traditionally rely on Makefiles or build systems like Ninja, both of which require a nontrivial understanding of their syntax to author. CMake simplifies this by generating build files for various build systems, including Make and Ninja, from a higher-level project description.\nThe best way to understand the value of CMake is through a concrete example.\nBuilding a single file # Note\nI worked through this example in Debian via WSL. I installed dependencies via:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade sudo apt install build-essential sudo apt install cmake Let\u0026rsquo;s say I have this uninteresting file, sum.c:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { int a, b, sum; printf(\u0026#34;Enter two space-separated numbers: \u0026#34;); sum = a + b; printf(\u0026#34;The sum is %d\\n\u0026#34;, sum); return 0; } I can compile this file into an executable with gcc sum.c -o sum, but I can also use CMake for my build. Here\u0026rsquo;s what a minimum viable CMakeList.txt for this file would look like:\ncmake_minimum_required(VERSION 3.25) project(Sum) add_executable(Sum sum.c) We specify a minimum required CMake version with cmake_minimum_required(). CMake has deprecated and added various features over its lifetime, and this setting says, \u0026ldquo;if you\u0026rsquo;re not updated to this version, builds won\u0026rsquo;t work.\u0026rdquo; project() defines my project\u0026rsquo;s name. add_executable({project_name} {sources_filename}) sets the sources files that should be built to create my project. Note\nLike CMake, the C++ standard has changed significantly over time. CXX_STANDARD can be used to specify which C++ standard your project depends on. I\u0026rsquo;ve excluded this property from my CMakeList.txt because my sample project is written in C.\nFrom the root of our project directory, we run CMake with: cmake .\nCMake will output something like:\n-- The C compiler identification is GNU 12.2.0 -- The CXX compiler identification is GNU 12.2.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /home/gibby/repos/math If I check my project directory, I\u0026rsquo;ll see that CMake has generated a bunch of files: CMakeCache.txt, cmake_install.cmake, a CMakeFiles directory, and a Makefile. You don‚Äôt need to worry about the meaning or contents of these files - that‚Äôs the magic of CMake. Given a declarative CMakeLists.txt, CMake generates all the necessary configuration to compile an executable. No build system expertise is needed.\nTip\nIf you\u0026rsquo;re using git, you shouldn\u0026rsquo;t check-in these generated files. Here\u0026rsquo;s how I exclude CMake files in my .gitignore:\nbuild/ CMakeCache.txt CMakeFiles/ cmake_install.cmake Makefile Next, run: cmake --build .\nCMake will output something like:\n[ 50%] Building C object CMakeFiles/Sum.dir/sum.c.o [100%] Linking C executable Sum [100%] Built target Sum At this point, I can run my built executable with ./sum. I have achieved bare minimum CMake proficiency!\nBuilding several files # Let\u0026rsquo;s say that I want to extend my project to support subtract alongside sum. I define main.c like this:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; void sum(); // Updated sum.c to contain void sum() instead of int main(void) void subtract(); int main(int argc, char *argv[]) { if (strcmp(argv[1], \u0026#34;sum\u0026#34;) == 0) { sum(); } else if (strcmp(argv[1], \u0026#34;sub\u0026#34;) == 0) { subtract(); } else { fprintf(stderr, \u0026#34;Error: Invalid argument. Use \u0026#39;sum\u0026#39; or \u0026#39;sub\u0026#39;.\\n\u0026#34;); return 1; } return 0; } (We\u0026rsquo;ll say that subtract.c is as uninteresting as sum.c is.)\nAt this point, if I rerun cmake . then nothing happens. As far as CMake is concerned, sum.c still exists so my CMakeLists.txt is still technically valid. But if I attempt to rerun cmake --build, I\u0026rsquo;ll hit these nasty errors:\n/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/Scrt1.o: in function `_start\u0026#39;: (.text+0x17): undefined reference to `main\u0026#39; collect2: error: ld returned 1 exit status gmake[2]: *** [CMakeFiles/Sum.dir/build.make:97: Sum] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/Sum.dir/all] Error 2 gmake: *** [Makefile:91: all] Error 2 Confusing and scary! CMake is complaining because sum.c no longer contains main(), so it can\u0026rsquo;t be used to create an executable anymore. We need to update CMakeLists.txt to fix this. The change is small:\ncmake_minimum_required(VERSION 3.25) project(Math) add_executable(Math sum.c subtract.c main.c) # No commas! Obviously we don\u0026rsquo;t want to manually update this config every time we add a new file to our project. Let\u0026rsquo;s rearrange the project like this:\n‚îî‚îÄ‚îÄ üìÅsrc ‚îî‚îÄ‚îÄ CMakeLists.txt ‚îî‚îÄ‚îÄ main.c ‚îî‚îÄ‚îÄ subtract.c ‚îî‚îÄ‚îÄ sum.c ‚îî‚îÄ‚îÄ CMakeLists.txt The root-level CMakeLists.txt contains:\ncmake_minimum_required(VERSION 3.25) project(Math) add_subdirectory(src) add_subdirectory() is a new command. It tells CMake, \u0026ldquo;the project will contain stuff in src, go read the CMakeLists.txt over there.\u0026rdquo;\nsrc/CMakeLists.txt contains:\nfile(GLOB SOURCES \u0026#34;*.c\u0026#34;) add_executable(Math ${SOURCES}) set_target_properties( Math PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR} ) The file(GLOB) line means, \u0026ldquo;find all files in this directory that match *.c and store them as variable SOURCES.\u0026rdquo; This will contain all of the .c files in src. Just like we had in our simple example, add_executable() is used to tell CMake which files are needed to build the Math project. We pass in the ${SOURCES} variable instead of a hard-coded list of files. The final line updates the target properties of Math so that the Math executable is output to ${CMAKE_BINARY_DIR}, which is the root dir of our project. If you omit this line, then running cmake --build . from the root will output Math into /src. Tip\nThis change has CMake creating an executable named Math, but I still have old Sum executables sitting in my workspace. cmake --build . --clean-first can be used to cleanup old CMake artifacts prior to building.\nLinking libraries # As a final extension to this Math project, let\u0026rsquo;s define a new util library. We can use this library to store common functions like isEven() or isPrime().\nWe add util to the project like this:\n‚îî‚îÄ‚îÄ üìÅsrc ‚îî‚îÄ‚îÄ üìÅutil ‚îî‚îÄ‚îÄ CMakeLists.txt ‚îî‚îÄ‚îÄ util.c ‚îî‚îÄ‚îÄ util.h ‚îî‚îÄ‚îÄ CMakeLists.txt ‚îî‚îÄ‚îÄ main.c ‚îî‚îÄ‚îÄ subtract.c ‚îî‚îÄ‚îÄ sum.c ‚îî‚îÄ‚îÄ CMakeLists.txt The contents of util.c and util.h are uninteresting, but the contents of src/util/CMakeLists.txt are important:\nfile(GLOB UTIL_SOURCES \u0026#34;*.c\u0026#34;) add_library(util ${UTIL_SOURCES}) This is pretty similar to src/CMakeLists.txt, but we use add_library instead of add_executable. We consider this util subdirectory to be a library because we want to use this code in the Math executable; we don\u0026rsquo;t want to produce separate Math and Util executables. I don\u0026rsquo;t specify a special RUNTIME_OUTPUT_DIRECTORY because I personally don\u0026rsquo;t care where the library ends up, as long as it\u0026rsquo;s accessible to Math.\nsum.c and subtract.c can access the functionality in util via #include \u0026quot;util/util.h\u0026quot;, but we still need to update src/CMakeLists.txt to compile the executable successfully. Here are the necessary changes:\nfile(GLOB SOURCES \u0026#34;*.c\u0026#34;) add_subdirectory(util) add_executable(Math ${SOURCES}) target_link_libraries(Math util) set_target_properties( Math PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR} ) We set add_subdirectory(util) to ensure that CMake processes src/util/CMakeLists.txt during compilation, just like we set add_subdirectory(src) from the root. Then we use target_link_libraries() to tell CMake, \u0026ldquo;make sure you link Math to this util library\u0026rdquo;.\nWith these changes, output of cmake --build . looks like this:\ncmake --build . [ 16%] Building C object src/util/CMakeFiles/util.dir/util.c.o [ 33%] Linking C static library libutil.a [ 33%] Built target util [ 50%] Linking C executable ../Math [100%] Built target Math And that\u0026rsquo;s about it!\nCMake has a lot of features and can be used to generate highly complex build systems, but as a C/C++ hobbyist, this is basically all that I\u0026rsquo;ve ever needed from CMake. I usually avoid using heavyweight IDEs, and CMake has made C development a lot more accessible to me.\n","date":"22 February 2025","externalUrl":null,"permalink":"/posts/cmake_starter/","section":"Posts","summary":"CMake made slightly simpler","title":"Getting started with CMake","type":"posts"},{"content":"","date":"18 February 2025","externalUrl":null,"permalink":"/tags/intermediate/","section":"Tags","summary":"","title":"Intermediate","type":"tags"},{"content":"I recently completed a homework assignment (inspired by blog posts like this one) that had us solving Advent of Code puzzles using SQL. This may sound like medieval torture, but once I worked through a few problems I found myself having\u0026hellip; fun? SQL is deceptively expressive, and adapting its semantics to Advent of Code problems feels like a brain teaser within a brain teaser.\n2024 Day 1 # This is a simple puzzle, so it‚Äôs a useful example for understanding how these SQL scripts can be structured. The problem asks us to read numbers into two ordered lists and return the absolute difference between list_1[x] and list_2[x] for all x.\nHere\u0026rsquo;s how I originally solved this problem using Python:\ndef solve(left: list[str], right: list[str]) -\u0026gt; int: lows, highs = sorted(left), sorted(right) return sum([abs(int(high) - int(low)) for low, high in zip(lows, highs)]) if __name__ == \u0026#34;__main__\u0026#34;: with open(\u0026#34;input/day1.txt\u0026#34;) as f: lines = [x.rstrip() for x in f] left, right = map(list, zip(*(s.split() for s in lines))) print(solve(left, right)) To start, I manually created a .csv of my puzzle input:\n0,3 4 1,4 3 2,2 5 3,1 3 4,3 9 5,3 3 I\u0026rsquo;m including an index with each row - not totally necessary for this problem, but useful for others. I wrote a small Python script to set this index on my actual 1000 line puzzle input.\nTo read input, each of my SQL scripts start like this:\n-- I use \u0026#39;drop table if exists\u0026#39; everywhere to keep scripts idempotent drop table if exists input_lines; create table input_lines (row int, line text); \\set filename :filepath \u0026#39;/\u0026#39; :filename copy input_lines from :\u0026#39;filename\u0026#39; delimiter \u0026#39;,\u0026#39;; There are probably other ways to do this (like COPY commands or manual INSERT INTO calls), but this works for me.\nAt this point, input_lines consists of rows of line and their row index. We\u0026rsquo;ve basically done the work of my Python code\u0026rsquo;s lines = [x.rstrip() for x in f]. Now we need to get these lines into a representation that\u0026rsquo;s actually useful for solving this puzzle. In Python, I split each line into left, right pairs. I can do something similar in SQL:\ndrop table if exists pairs; create table pairs ( row int, x int, y int ); insert into pairs (row, x, y) select row, split_part(line, \u0026#39; \u0026#39;, 1)::int as x, -- split line, take first element split_part(line, \u0026#39; \u0026#39;, 2)::int as y -- split line, take second element from input_lines; select * from pairs; returns:\nrow | x | y -----+---+--- 0 | 3 | 4 1 | 4 | 3 2 | 2 | 5 3 | 1 | 3 4 | 3 | 9 5 | 3 | 3 In Python, I sorted left and right, zipped them together, and summed the absolute difference between all pairs. This is doable in SQL too:\nwith sorted_x as ( SELECT row_number() over (order by x) AS rn, x from pairs ), sorted_y as ( select row_number() over (order by y) AS rn, y from pairs ), diffs as ( select sx.x, sy.y, abs(sx.x - sy.y) as diff from sorted_x sx, sorted_y sy where sx.rn = sy.rn ) select sum(diff) as answer from diffs; For the example puzzle input, this correctly returns:\nanswer -------- 11 (1 row) Note\nI set up my Postgres database using: createdb aoc\nThen I ran my script using: psql -q -U postgres -f aoc01.sql -v filepath='/data' -v filename='aoc01_input.csv' aoc\nI was curious about performance, so I ran with /timing on my actual puzzle input and found that my SQL script runs in 2.3ms. Not as bad as I expected, but my Python implementation is still much faster at 0.9ms.\nAs a language designed for handling relational data, SQL isn\u0026rsquo;t well-suited for array problems like these. SQL\u0026rsquo;s true puzzle solving power becomes more obvious when we start looking at graph + grid problems.\n2022 Day 8 # This puzzle is your typical Advent of Code grid problem. We\u0026rsquo;re asked to count visible trees in a grid of tree heights. All trees along grid edges are visible. Interior trees are visible if all neighbors have lower values. Count up the visible trees.\nInfo\nI originally solved this problem in Rust using a brute force approach. My code is a 150 line mess, so I won\u0026rsquo;t paste it in its entirety here.\nI had the foresight to document my high-level approach at the top:\n/// this one took me the longest so far. it was a nightmare /// 2d vec was a mistake. /// using iter was a mistake. should\u0026#39;ve just used slices and indices /// this is accomplished in a psychotic way /// using the \u0026#39;die\u0026#39; vars as semaphore-like things to keep my sums clean /// anyway i hated every second of it. So much to unpack. A tormented soul that unwittingly yearns for SQL.\nWe read puzzle input into input_lines just as we did for the previous problem. Here\u0026rsquo;s how we read input_lines into a grid[x][y]:\ndrop table if exists grid; create table grid ( row int, col int, val text ); with unsplit as ( select row, generate_series(1, length(line)) as col, line from input_lines ) insert into grid (row, col, val) select row, col, substring (line from col for 1) from unsplit; -- If you want to exclude cells from the grid (because they contain obstructions or something) -- you can do that with a WHERE clause here. We use substring to split each line into characters and we use generate_series to set column indices on each character.\nIt\u0026rsquo;s easy enough to write a query that counts visible trees along the edge of our grid:\nwith max_row as ( select max(row) as max_row from grid ), max_col as ( select max(col) as max_col from grid ), edges as ( select row, col, val from grid, max_row as mr, max_col as mc where row = 1 or row = mr.max_row or col = 1 or col = mc.max_col ) But how are we supposed to count visible trees in the grid\u0026rsquo;s interior? To proceed further, we need to learn more about recursive queries.\nAside: recursive queries # In SQL, a recursive query is generally shaped like this:\nwith recursive result as ( -- base case select x from product where price \u0026gt; 0 -- can also be UNION ALL union -- recursive case select r.x, s.y from result r, supply y where condition ) The base query forms our initial result set. The recursive query defines how we want to grow our initial result set. Each set is combined via union. The recursive query will execute until we stop generating new rows in the overall result set. This implicit termination condition is subtle - I found it difficult to understand at first, but it\u0026rsquo;s core to the design of recursive queries in SQL.\nNote\nThe recursive query\u0026rsquo;s termination condition means that the choice between union and union all can have massive implications for your query\u0026rsquo;s correctness.\nRemember:\nunion removes duplicate records. union all does not remove duplicate records. If you\u0026rsquo;re using a recursive query to solve a path finding problem, union will eliminate duplicate visits from your result set. union all will keep them, potentially allowing your path finding query to run forever.\nThis is easily solvable if we update our base case to contain some counter variable, iterate it in the recursive case, and include where count \u0026lt; {limit} in the recursive condition. I think it\u0026rsquo;s simpler to select the best union from the outset.\nRecursive queries can become even more expressive if we use forms like this:\nwith recursive result as ( select row as curr_row from grid where row = 0 union select case when g.curr_row = (select max(row) from grid) then g.curr_row - 1 -- If at max row, move down else g.curr_row + 1 -- else, move down end as curr_row, from result r, grid g where r.curr_row = g.row ) With select case when, we warp the powers of recursive queries such that we end up with something\u0026hellip; vaguely iterative? We don\u0026rsquo;t need to use this technique for 2022 Day 8, but I wanted to mention this technique because I really feel like it\u0026rsquo;s the key to unlocking SQL\u0026rsquo;s Advent of Code potential. It‚Äôs messy, but it helped me conceptualize SQL solutions I wouldn‚Äôt have considered possible before.\nBack to 2022 Day 8 # Here\u0026rsquo;s the recursive query that I used to solve 2022 Day 8:\nwith recursive trees as ( -- edge trees are always visible select row, col, val from grid where row = 1 or col = 1 or row = (select max(row) from grid) or col = (select max(col) from grid) union -- check interior trees against their neighbors select g.row, g.col, g.val from grid g join trees t on ( (g.row = t.row + 1 and g.col = t.col) or (g.row = t.row - 1 and g.col = t.col) or (g.row = t.row and g.col = t.col + 1) or (g.row = t.row and g.col = t.col - 1) ) where g.val \u0026gt; t.val ) select count(*) as answer from trees; It\u0026rsquo;s almost magical, isn\u0026rsquo;t it? Our initial set consists of all edges, and in the recursive case we check trees adjacent to known-visible trees. The output set will contain the edges and all interior cells that passed the recursive condition.\nAlthough my SQL solution is obviously slower than my Rust solution, I find it amusing that my SQL solution is 1/3 the length of my Rust code. SQL is also easier to debug in some cases. If your result is too large or too small, it\u0026rsquo;s easy to select * pretty print the output of each intermediate query + subquery.\nI\u0026rsquo;ve always hated solving grid problems with normal programming languages; there are so many indices and conditions to track, and mistakes are difficult to debug. This might not be practical knowledge for everyone, but next time I‚Äôm struggling with a flood fill problem during Advent of Code, I‚Äôll consider reaching for SQL.\n","date":"18 February 2025","externalUrl":null,"permalink":"/posts/sql_aoc/","section":"Posts","summary":"Anything can be a general purpose language if you believe in yourself","title":"Solving Advent of Code with SQL","type":"posts"},{"content":"","date":"18 February 2025","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"SQL","type":"tags"},{"content":"I\u0026rsquo;ve been having a blast with Svelte 5 but it took me a sec to figure out how to get global state that \u0026ldquo;just works\u0026rdquo; with runes. So far, this is my favorite pattern:\n// src/lib/state/user.svelte.ts const emptyUser = { username: \u0026#34;\u0026#34;, email: \u0026#34;\u0026#34;, profile: {}, createdAt: new Date(), updatedAt: new Date(), }; const loggedInUser = await getSession() ?? emptyUser; export const cUser = user(loggedInUser); function user(init: User) { // fns modifying User let user = $state(init); return { get get() { return { ...user }; }, set set(newUser: User) { user = newUser; }, set profile(profile: Profile) { user.profile = profile; }, get username() { return user.username; }, get email() { return user.email; }, get profile() { return user.profile; } }; } Access the state like this:\n\u0026lt;script\u0026gt; import { cUser } from \u0026#39;$lib/state/user.svelte\u0026#39;; let user: User | Record\u0026lt;string, never\u0026gt; = cUser.get; \u0026lt;/script\u0026gt; \u0026lt;h3 class=\u0026#34;h3\u0026#34;\u0026gt;Welcome back, {user.username}!\u0026lt;/h3\u0026gt; ","date":"1 January 2025","externalUrl":null,"permalink":"/posts/svelte_state/","section":"Posts","summary":"Svelte state for dummies (it\u0026rsquo;s me, I\u0026rsquo;m dummies)","title":"Global state in Svelte 5","type":"posts"},{"content":"","date":"1 January 2025","externalUrl":null,"permalink":"/tags/svelte/","section":"Tags","summary":"","title":"Svelte","type":"tags"},{"content":"","date":"1 January 2025","externalUrl":null,"permalink":"/tags/tip/","section":"Tags","summary":"","title":"Tip","type":"tags"},{"content":" Apparently, the gateway drug to kernel development Two years ago, I came across this excellent tutorial written by snaptoken. It walks through the process of building a small text editor in C. I don\u0026rsquo;t really enjoy following along with code tutorials verbatim, so I thought it would be a fun challenge to use snaptoken\u0026rsquo;s tutorial as a general reference and implement the text editor in Rust instead. I effectively treated the tutorial like a product manager and came away from each chapter with a new list of features to implement:\nChapter 2 # Enable raw mode. Write each stdin char to stdout. Ctrl indicates a carriage return. q indicates that the user wants to quit. Ctrl-# combinations (e.g., Ctrl-Z) should be ignored. Chapter 3 # Clear the screen on startup, shutdown, and on tick. Draw a tilde border on the left-hand side of the screen. Render a cursor in the first position of the first line. Replace println!() with write!()s to a BufWriter. Display a welcome message at the bottom of the screen. Allow the user to move their cursor using arrow keys. Cursor info should be stored in global state. Don\u0026rsquo;t allow the cursor to move offscreen. Page Up and Page Down can snap the cursor to the top and bottom of the screen. Home and End can snap the cursor to the beginning and end of the current line. Chapter 4 # If run with a filename argument, read the file into memory. Enable vertical scrolling through a long text file. Enable horizontal scrolling on long lines. (Basically, allow users to scroll offscreen if the text allows it.) Snap to the next line if the user scrolls past the end of a line. Snap to the previous line if the user scrolls left at the beginning of a line. Update Page Up and Page Down to scroll up and down the entire page. Display a status bar that includes the filename, # of lines in the file, and current line number. If the file has no name, display [No Name]. Render key binding hints in the status bar (e.g., Ctrl-Q = quit). Display on startup and hide 5 seconds after the user presses a key. Chapter 5 # Allow saving to disk via Ctrl-S. Track \u0026ldquo;dirtiness\u0026rdquo; in global state. The buffer is \u0026ldquo;dirty\u0026rdquo; if it has been modified since opening or saving the file. Require Ctrl-Q 3x to confirm quitting with unsaved changes. Implement Backspace. Backspacing at the beginning of a line should wrap the line onto the previous line. If there is no previous line, do nothing. Implement Delete. Deleting at the end of a line should wrap onto the next line. If there is no next line, do nothing. Implement Enter. This can insert new lines or split a line into two lines. Prompt the user to input a filename when saving a new file. Allow Backspace in prompt input. Chapter 6 # Implement search with Ctrl-F. The user inputs their query into the prompt bar. On each keypress, the cursor snaps to a string that contains their query substring. When search is cancelled with Escape, restore the cursor to its original position. Users can browse through substring matches using arrow keys. Left/Up to move to the previous match, Right/Down to move to the next match. Chapter 7 # Implement basic C syntax highlighting. Digits are red. Search results are blue. Strings are magenta. Single-line and multi-line comments are cyan. Types are green. Keywords are yellow. Syntax highlighting should only appear if the open filename ends with .c. Search result highlighting should overwrite other highlighting. Original highlighting should be restored when search is cancelled. Crates I used # termion: This looked like the simplest way to enable raw mode without C interop. (Avoiding C interop was my only self-imposed rule for this project; after all, I wanted to have fun \u0026#x1f60b;) Cursor rendering and text rendering also met all my requirements and was simple enough to use. termsize: I used this to get terminal window size on startup. (I never actually updated my implementation to handle mid-session window resizes\u0026hellip;) unicode_segmentation: I used this to split strings on grapheme cluster boundaries instead of character boundaries. This ensures that a character with a diacritic like √§ is kept together as a single element instead of split apart (also allows the text editor to support other exotic Unicode input like emoji, Cyrillic, etc.) My experience # I had a heck of a time with this project overall. I dropped it twice: once in 2022 because I was intimidated by Chapter 6, and another time in mid 2023 because I was intimidated by Chapter 7 and sick of looking at my years old over-engineered code.\nIt didn\u0026rsquo;t take long for my code to diverge significantly from snaptoken\u0026rsquo;s code. It started with, \u0026ldquo;oh, well I can\u0026rsquo;t really do that in Rust, so I have to do it this other way instead,\u0026rdquo; and ended with, \u0026ldquo;yeahhh, my design is nothing like that, I have to do this differently\u0026rdquo;.\nThe final structure that I ended up with is mind-boggling, to be honest:\nüìÇsrc ‚î£ üìÇbackend ‚îÉ ‚î£ cursor.rs ‚îÉ ‚î£ mod.rs ‚îÉ ‚î£ operations.rs ‚îÉ ‚îó prompt.rs ‚î£ üìÇdata ‚îÉ ‚î£ enums.rs ‚îÉ ‚î£ mod.rs ‚îÉ ‚î£ payload.rs ‚îÉ ‚îó textrow.rs ‚î£ üìÇgfx ‚îÉ ‚î£ controller.rs ‚îÉ ‚î£ mod.rs ‚îÉ ‚îó render.rs ‚î£ input.rs ‚î£ main.rs ‚îó utils.rs The RenderController owns a CursorHandler and an OperationsHandler. The OperationsHandler is a middle layer between the RenderController, RenderDriver, and PromptProcessor. Lol?\nThis object-oriented approach must have been veryyy satisfying for me to build out in 2022, but it quickly became a rat king as I had to wrestle with issues like: \u0026ldquo;how do I pass data from the PromptProcessor to its parent\u0026rsquo;s parent RenderController!?!?\u0026rdquo; If implementing a text editor is truly a gateway drug into kernel development, then I overdosed.\nKilo is considered a simple project because it can be implemented with ~1000 lines of code. Running tokei on my repo tells a grim tale:\n=================================================================== Language Files Lines Code Comments =================================================================== C 1 28 18 4 Markdown 1 13 0 10 Rust 14 1897 1410 299 TOML 1 12 9 1 =================================================================== Total 17 1950 1437 314 =================================================================== You\u0026rsquo;re invited to gawk at my repo for yourself:\ngibbyfree/gram Rust kilo implementation Rust 0 0 Over-abstraction aside, I had a lot of fun implementing all of these features. It\u0026rsquo;s easy to dream up a bunch of potential extensions to Kilo. I\u0026rsquo;ve had enough of text editor development for the time being \u0026#x1f642; but I was tempted to implement automated testing. It was tedious to get through Chapter 6 + 7 when large code changes required me to manually check for regressions in all previously implemented features. Someday, I might try compiling my code to WASM or rebuilding with xterm-js-rs so I can get the app web-embeddable.\n","date":"28 December 2024","externalUrl":null,"permalink":"/posts/gram/","section":"Posts","summary":"Apparently, the gateway drug to kernel development","title":"Building a basic text editor in Rust","type":"posts"},{"content":"","date":"28 December 2024","externalUrl":null,"permalink":"/tags/project/","section":"Tags","summary":"","title":"Project","type":"tags"},{"content":"","date":"28 December 2024","externalUrl":null,"permalink":"/tags/rust/","section":"Tags","summary":"","title":"Rust","type":"tags"},{"content":"","date":"17 May 2024","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"Devops","type":"tags"},{"content":"","date":"17 May 2024","externalUrl":null,"permalink":"/tags/guide/","section":"Tags","summary":"","title":"Guide","type":"tags"},{"content":" If a dockerfile is built in a container and no one is around to see it, does it make a sound? Recently, I\u0026rsquo;ve been grappling with Docker containers. When I build a dockerfile via docker build, my host machine has limited visibility into the containerized build process.\nThis is by Docker design, but still presents a challenge if I want to monitor Docker builds at scale.\nI can move monitoring into the dockerfile itself, but then I run into another issue - how do I pull relevant artifacts out of the resulting Docker image? Bonus points if I can extract these artifacts without running Docker.\nInfo\nA Dockerfile contains a sequence of build instructions executed within an isolated (containerized) environment. These instructions generate a Docker image, which serve as a blueprint for container creation.\nWhen an image is executed, Docker creates a container based on the image, effectively reproducing the environment defined in the Dockerfile.\nEach instruction in a Dockerfile corresponds to an image layer. A given layer contains the delta between the current layer and the previous one, usually representing a change in the underlying container\u0026rsquo;s file system or configuration.\nSetting up a Docker image # Let\u0026rsquo;s start by building a Docker image. Here\u0026rsquo;s my motivating example:\nFROM rust:1.76.0-alpine3.18 RUN apk update \u0026amp;\u0026amp; apk add strace COPY . . WORKDIR /app RUN strace -o trace.log cargo run This dockerfile copies an application into the Docker build context and traces the application\u0026rsquo;s build with strace. strace will output its logfile to trace.log.\nLet\u0026rsquo;s say I\u0026rsquo;m tracing my build with strace because my Rust project is open-source and frequently integrates new open-source dependencies. To protect myself from supply chain attacks, I want to watch for suspicious network calls when my app is executed. The only way to collect this information from the Docker build is by running strace from within the container.\nThe problem # \u0026hellip;But then how do we actually get our hands on trace.log? We could docker run the container and search its file system manually. But what if I\u0026rsquo;m trying to integrate this check into my CI pipeline? Can this process be automated?\nThis general problem applies to any scenario where your Dockerfile produces some artifact that you\u0026rsquo;d like to extract from the resulting container.\nI\u0026rsquo;m personally interested in this tracing example because a command like strace -o trace.log docker build -f Dockerfile . doesn\u0026rsquo;t produce an interesting trace.log, due to the containerized build process that I mentioned earlier.\nThe solution # First, build and save the Docker image:\ndocker build -f Dockerfile -t myimage:1.0 . docker save -o myimage.tar myimage:1.0 This docker build command builds the Dockerfile in the current directory (.) and tags the resulting image with myimage:1.0. docker save compresses the image to a .tar file - these .tar files are typically used to distribute, deploy, and/or backup Docker images. For my case, docker save will save the image in a more easily dissectible format.\nI\u0026rsquo;ll extract the image tarball to a clean output directory:\nmkdir myimage tar -xvf myimage.tar -C myimage Once extracted, here are the contents of myimage:\nüìÅmyimage ‚îî‚îÄ‚îÄ üìÅblobs ‚îî‚îÄ‚îÄ üìÅsha256 ‚îî‚îÄ‚îÄ 1fc818d7122c2955f10c33a86a84be0585dce8264f07ff20d26d6e4d77072689 ‚îî‚îÄ‚îÄ 5500516daa5d159d01f04ea168438de6cbc4a86197cf3b462b7f00e1054f1fe6 ‚îî‚îÄ‚îÄ 5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef ‚îî‚îÄ‚îÄ 60be3224167df7d3927eef73eab4d9fbac87c2ad227db7302f62c715ee1aedcf ‚îî‚îÄ‚îÄ 79643e31b5e64ee2c97c3fada467f944d97691b15beeabd27d60bb24dcf62958 ‚îî‚îÄ‚îÄ 807a30def12c95b7638d81513f8ff014b82e0be9b02c3a2741a2de417afff353 ‚îî‚îÄ‚îÄ aedc3bda2944bb9bcb6c3d475bee8b460db9a9b0f3e0b33a6ed2fd1ae0f1d445 ‚îî‚îÄ‚îÄ b7da1d95051fa9e21f5c9934d4abb8686038e4d76775c857a39644fc3a7b8e81 ‚îî‚îÄ‚îÄ b88b8a4eae96dcec86268cf8c5b53dac6c263430682a7f1705e35eb34f5b4a5a ‚îî‚îÄ‚îÄ cb35dc109a5e6c8f76a05a26ae1178fc7489ddf690110600dc0b60ea05d371c0 ‚îî‚îÄ‚îÄ d708bd3ae8e28161aa0fa6207912621fd352c1126593347982cde1a2e609ac14 ‚îî‚îÄ‚îÄ dcc8140df88f28d0f2807d30bae4a76c2d597e1dda6f98274d37c5bfc0e9dd06 ‚îî‚îÄ‚îÄ dd4e2e76f6fc4791b1dd86cee52df664163fd809ed59bd9eb2181a4db94b65a2 ‚îî‚îÄ‚îÄ e78fc723f9dec6ad6c8cdac73bd8df9c477b3fe62741df0b2e6112d47b35c132 ‚îî‚îÄ‚îÄ f0a1565b3b4d7c42ab9ad5f613fecdf9c84a493e5c31ee7c155b45e74b5f17d4 ‚îî‚îÄ‚îÄ f53c508663464b69bf26e90bc5d916b171aa7c946ddc9c6b761aeaed2839996e ‚îî‚îÄ‚îÄ index.json ‚îî‚îÄ‚îÄ manifest.json ‚îî‚îÄ‚îÄ oci-layout ‚îî‚îÄ‚îÄ repositories Each of these (extensionless) hashed files under blobs/sha256 represent a layer in the Docker image. I\u0026rsquo;m guaranteed to find trace.log somewhere in this forest of layers.\nNaive approach # Knowing this, I might try to find strace.log with brute force. I can just extract every layer under blobs/sha256, then recursively search all child directories for strace.log.\nI can try automating this procedure with a script like this:\n#!/bin/bash BLOBS_DIR=\u0026#34;./myimage/blobs/sha256\u0026#34; # Attempt extraction on all extensionless files for file in \u0026#34;$BLOBS_DIR\u0026#34;/*; do if [ -f \u0026#34;$file\u0026#34; ]; then echo \u0026#34;Attempting extraction of $file...\u0026#34; tar -xf \u0026#34;$file\u0026#34; -C \u0026#34;$BLOBS_DIR\u0026#34; fi done # Search for trace.log in the blobs directory find \u0026#34;$BLOBS_DIR\u0026#34; -type f -name \u0026#34;trace.log\u0026#34; When I run this script, I encounter this error for a few of the layers: tar: This does not look like a tar archive but the script continues on and trace.log is eventually located. Yay!\nAlthough this approach works for this simple case, there are some drawbacks.\nMy image is based on a lightweight alpine image, but if I were using a beefier ubuntu image then this script might take a very long time to execute. This is because the base image itself is also contained within blobs/sha256, so locating trace.log involves extracting layers that contain the image base\u0026rsquo;s file system.\nNote\nWhen experimenting with this approach in a CI environment, I wanted to cleanup $BLOBS_DIR to prevent the unpacked tarball from leaking into my build artifacts. I ran into ownership-related errors when trying to delete base image files form $BLOBS_DIR. This may have been an issue specific to my build system though.\nBetter approach # It\u0026rsquo;s possible to locate strace.log and extract a single layer of the image - i.e., no unnecessary extractions. To do this, we need to leverage some knowledge of the OCI Image Specification to decode the contents of myimage.tar.\nFirst, I\u0026rsquo;ll reference the OCI image manifest spec to understand more about manifest.json. My image\u0026rsquo;s manifest.json looks like this:\n[ { \u0026#34;Config\u0026#34;: \u0026#34;blobs/sha256/b88b8a4eae96dcec86268cf8c5b53dac6c263430682a7f1705e35eb34f5b4a5a\u0026#34;, \u0026#34;RepoTags\u0026#34;: [ \u0026#34;myimage:1.0\u0026#34; ], \u0026#34;Layers\u0026#34;: [ \u0026#34;blobs/sha256/aedc3bda2944bb9bcb6c3d475bee8b460db9a9b0f3e0b33a6ed2fd1ae0f1d445\u0026#34;, \u0026#34;blobs/sha256/5500516daa5d159d01f04ea168438de6cbc4a86197cf3b462b7f00e1054f1fe6\u0026#34;, \u0026#34;blobs/sha256/cb35dc109a5e6c8f76a05a26ae1178fc7489ddf690110600dc0b60ea05d371c0\u0026#34;, \u0026#34;blobs/sha256/79643e31b5e64ee2c97c3fada467f944d97691b15beeabd27d60bb24dcf62958\u0026#34;, \u0026#34;blobs/sha256/1fc818d7122c2955f10c33a86a84be0585dce8264f07ff20d26d6e4d77072689\u0026#34;, \u0026#34;blobs/sha256/5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34;, \u0026#34;blobs/sha256/d708bd3ae8e28161aa0fa6207912621fd352c1126593347982cde1a2e609ac14\u0026#34; ], \u0026#34;LayerSources\u0026#34;: { \u0026#34;sha256:1fc818d7122c2955f10c33a86a84be0585dce8264f07ff20d26d6e4d77072689\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 103592960, \u0026#34;digest\u0026#34;: \u0026#34;sha256:1fc818d7122c2955f10c33a86a84be0585dce8264f07ff20d26d6e4d77072689\u0026#34; }, \u0026#34;sha256:5500516daa5d159d01f04ea168438de6cbc4a86197cf3b462b7f00e1054f1fe6\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 138808320, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5500516daa5d159d01f04ea168438de6cbc4a86197cf3b462b7f00e1054f1fe6\u0026#34; }, \u0026#34;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 1024, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34; }, \u0026#34;sha256:79643e31b5e64ee2c97c3fada467f944d97691b15beeabd27d60bb24dcf62958\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 4128256, \u0026#34;digest\u0026#34;: \u0026#34;sha256:79643e31b5e64ee2c97c3fada467f944d97691b15beeabd27d60bb24dcf62958\u0026#34; }, \u0026#34;sha256:aedc3bda2944bb9bcb6c3d475bee8b460db9a9b0f3e0b33a6ed2fd1ae0f1d445\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 7629824, \u0026#34;digest\u0026#34;: \u0026#34;sha256:aedc3bda2944bb9bcb6c3d475bee8b460db9a9b0f3e0b33a6ed2fd1ae0f1d445\u0026#34; }, \u0026#34;sha256:cb35dc109a5e6c8f76a05a26ae1178fc7489ddf690110600dc0b60ea05d371c0\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 672828928, \u0026#34;digest\u0026#34;: \u0026#34;sha256:cb35dc109a5e6c8f76a05a26ae1178fc7489ddf690110600dc0b60ea05d371c0\u0026#34; }, \u0026#34;sha256:d708bd3ae8e28161aa0fa6207912621fd352c1126593347982cde1a2e609ac14\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;size\u0026#34;: 4483072, \u0026#34;digest\u0026#34;: \u0026#34;sha256:d708bd3ae8e28161aa0fa6207912621fd352c1126593347982cde1a2e609ac14\u0026#34; } } } ] Notice that there are far fewer layers listed under Layers than I saw in myimage/blobs/sha256 earlier. The manifest has filtered out any \u0026ldquo;empty\u0026rdquo; layers - i.e., layers that apply environmental changes but don\u0026rsquo;t modify the file system.\nFurthermore, the \u0026ldquo;layer\u0026rdquo; listed as the manifest\u0026rsquo;s Config isn\u0026rsquo;t a layer at all. This Config value is a pointer to the image\u0026rsquo;s config.json. (This Config path would have produced a tar: This does not look like a tar archive error like we saw with the naive approach.)\nI can read this image\u0026rsquo;s config.json by running this command:\ncat myimage/blobs/sha256/b88b8a4eae96dcec86268cf8c5b53dac6c263430682 a7f1705e35eb34f5b4a5a \u0026gt; config.json The config.json looks like this:\n{ \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;RUSTUP_HOME=/usr/local/rustup\u0026#34;, \u0026#34;CARGO_HOME=/usr/local/cargo\u0026#34;, \u0026#34;RUST_VERSION=1.76.0\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;/bin/sh\u0026#34; ], \u0026#34;WorkingDir\u0026#34;: \u0026#34;/app\u0026#34; }, \u0026#34;created\u0026#34;: \u0026#34;2024-05-13T17:06:51.253198519Z\u0026#34;, \u0026#34;history\u0026#34;: [ { \u0026#34;created\u0026#34;: \u0026#34;2024-01-27T00:30:56.150825642Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ADD file:8729f9c0258836b640e9e789c7ab029cf4547e0596557d54dd4a4d7d8e4a785f in / \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-01-27T00:30:56.304681072Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;/bin/sh\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2024-03-11T15:56:03Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;RUN /bin/sh -c apk add --no-cache ca-certificates gcc # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-03-11T15:56:03Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;ENV RUSTUP_HOME=/usr/local/rustup CARGO_HOME=/usr/local/cargo PATH=/usr/local/cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin RUST_VERSION=1.76.0\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2024-03-11T15:56:03Z\u0026#34;, \u0026#34;created_by\u0026#34;:\u0026#34;RUN /bin/sh -c set -eux; apkArch=\\\u0026#34;$(apk --print-arch)\\\u0026#34;; case \\\u0026#34;$apkArch\\\u0026#34; in x86_64) rustArch=\u0026#39;x86_64-unknown-linux-musl\u0026#39;; rustupSha256=\u0026#39;b9d84cbba1ed29d11c534406a1839d64274d29805041e0e096d5293ae6390dd0\u0026#39; ;; aarch64) rustArch=\u0026#39;aarch64-unknown-linux-musl\u0026#39;; rustupSha256=\u0026#39;841513f7599fcf89c71a62dea332337dfd4332216b60c17648d6effbeefe66a9\u0026#39; ;; *) echo \\u003e\\u00262 \\\u0026#34;unsupported architecture: $apkArch\\\u0026#34;; exit 1 ;; esac; url=\\\u0026#34;https://static.rust-lang.org/rustup/archive/1.27.0/${rustArch}/rustup-init\\\u0026#34;; wget \\\u0026#34;$url\\\u0026#34;; echo \\\u0026#34;${rustupSha256} *rustup-init\\\u0026#34; | sha256sum -c -; chmod +x rustup-init; ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${rustArch}; rm rustup-init; chmod -R a+w $RUSTUP_HOME $CARGO_HOME; rustup --version; cargo --version; rustc --version; # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-05-13T17:06:49.659900887Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;RUN /bin/sh -c apk update \\u0026\\u0026 apk add strace # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-05-13T17:06:49.823538715Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;COPY . . # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-05-13T17:06:49.852497547Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;WORKDIR /app\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2024-05-13T17:06:51.253198519Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;RUN /bin/sh -c strace -o trace.log cargo run # buildkit\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;buildkit.dockerfile.v0\u0026#34; } ], \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:aedc3bda2944bb9bcb6c3d475bee8b460db9a9b0f3e0b33a6ed2fd1ae0f1d445\u0026#34;, \u0026#34;sha256:5500516daa5d159d01f04ea168438de6cbc4a86197cf3b462b7f00e1054f1fe6\u0026#34;, \u0026#34;sha256:cb35dc109a5e6c8f76a05a26ae1178fc7489ddf690110600dc0b60ea05d371c0\u0026#34;, \u0026#34;sha256:79643e31b5e64ee2c97c3fada467f944d97691b15beeabd27d60bb24dcf62958\u0026#34;, \u0026#34;sha256:1fc818d7122c2955f10c33a86a84be0585dce8264f07ff20d26d6e4d77072689\u0026#34;, \u0026#34;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34;, \u0026#34;sha256:d708bd3ae8e28161aa0fa6207912621fd352c1126593347982cde1a2e609ac14\u0026#34; ] } } The most important part of this file is the history array. Each entry in the history array corresponds to a layer, and each entry\u0026rsquo;s created_by indicates which build command produced it. If a layer is created_by a command like touch readme.txt, then this layer .tar will contain readme.txt.\nIn the above history array, the entry where created_by contains \u0026ldquo;-o trace.log\u0026rdquo; describes a layer that contains trace.log. This is the precise layer that I want to extract.\nTo map each layer to its sha256 hash, filter out all layers where empty_layer=true. The remaining entries in history correspond to each hash listed in rootfs.diff_ids - meaning history[n] corresponds to rootfs.diff_ids[n].\nOnce I have the layer\u0026rsquo;s hash, I can proceed with extracting the layer and pulling out trace.log.\nI wrote a script which automates this entire process, given myimage.tar:\n#!/bin/bash # Extract myimage.tar to a clean directory mkdir myimage tar -xvf myimage.tar -C myimage # Write config.json configPath=$(jq -r \u0026#39;.[0].Config\u0026#39; myimage/manifest.json) cat myimage/$configPath \u0026gt; myimage/config.json # Read myimage/config.json and filter out empty_layer history entries history=$(jq -c \u0026#39;.history[] | select(.empty_layer != true)\u0026#39; myimage/config.json) historyArray=($(echo $history | jq -r \u0026#39;.[]\u0026#39;)) # Find the index of the history entry where created_by contains \u0026#39;-o strace.log\u0026#39; index=-1 for i in \u0026#34;${!historyArray[@]}\u0026#34;; do if [[ ${historyArray[i]} == *\u0026#34;-o strace.log\u0026#34;* ]]; then index=$i break fi done # Read rootfs.diff_ids and get the entry at the same index rootfs=$(jq -r \u0026#34;.rootfs.diff_ids[$index]\u0026#34; myimage/config.json) # Strip the sha256: prefix rootfs=$(echo $rootfs | sed \u0026#39;s/sha256://\u0026#39;) # Extract the trace.log\u0026#39;s layer into a clean directory mkdir $rootfs tar -xvf myimage/blobs/sha256/$rootfs -C $rootfs # Find trace.log in myimage/$rootfs and move it into this script\u0026#39;s directory script_dir=$(dirname \u0026#34;$0\u0026#34;) find $rootfs -name \u0026#39;trace.log\u0026#39; -exec mv {} \u0026#34;$script_dir\u0026#34; \\; # Cleanup rm -rf myimage rm -rf $rootfs This script is dependent on jq, which is a command-line JSON parser.\nPost-script: ye olde Docker image spec # Prior to adopting the OCI specification with Docker 1.11, Docker had its own image spec.\nMy original solution was written for the older Docker image spec, since I was working with build systems that used an older version of Docker. If you follow these steps with Docker \u0026lt;1.11, these .json files will be shaped a little differently. The same approach still works though.\nHere\u0026rsquo;s are the key differences that I noticed with the Docker image spec:\nHashes correspond to directories instead of files, and each hash directory contains a layer.tar. Hash directories are at the top-level of the image. config.json is a {sha256hash}.json file in the top-level of the tarball. Layer hashes can\u0026rsquo;t be translated using the rootfs.diff_ids. Once empty layers are filtered out, history[n] corresponds to manifest.json\u0026rsquo;s layers[n]. I\u0026rsquo;ve adopted the script above for Docker image specs here:\n#!/bin/bash # Extract myimage.tar to a clean directory mkdir myimage tar -xvf nc-janitor.tar -C myimage # Write config.json configPath=$(echo $manifestJson | jq -r \u0026#39;.[0].Config\u0026#39;) configJson=$(jq \u0026#39;.\u0026#39; $tempDir/$config) # Read myimage/config.json and filter out empty_layer history entries history=$(echo $configJson | jq -c \u0026#39;.history[] | select(.empty_layer != true)\u0026#39;) # Find the index of the history entry with -o strace.log historyArray=($(echo $history | jq -r \u0026#39;.[]\u0026#39;)) index=-1 for i in \u0026#34;${!historyArray[@]}\u0026#34;; do if [[ ${historyArray[i]} == *\u0026#34;codeql database create\u0026#34;* ]]; then index=$i break fi done # Find the layer tarball that corresponds to the index we found above layer=$(echo $manifestJson | jq -r \u0026#34;.layers[$index].digest\u0026#34;) # Extract the artifact\u0026#39;s layer into a clean directory mkdir output tar -xvf myimage/$layer/layer.tar -C $tempDir/$layer # Find trace.log in myimage/$rootfs and move it to the script\u0026#39;s directory script_dir=$(dirname \u0026#34;$0\u0026#34;) find $rootfs -name \u0026#39;trace.log\u0026#39; -exec mv {} \u0026#34;$script_dir\u0026#34; \\; # Cleanup rm -rf myimage rm -rf output (I\u0026rsquo;m not totally sure about the stability of Docker\u0026rsquo;s image spec, but this script works for me on image tarballs saved with Docker v1.2.)\n","date":"17 May 2024","externalUrl":null,"permalink":"/posts/docker_extraction/","section":"Posts","summary":"If a dockerfile is built in a container and no one is around to see it, does it make a sound?","title":"Static extraction of Docker image artifacts","type":"posts"},{"content":"I‚Äôm a software engineer at Microsoft and a master‚Äôs student in computer science.\nMy preferred programming languages include Rust, TypeScript, C#\u0026hellip; umm\u0026hellip;\nWhy blog? # I\u0026rsquo;m a firm believer in the Internet\u0026rsquo;s power to serve as a repository of information and expertise. I write here because I love to share my learnings with others, and because I believe that information should be accessible to everyone. I grew up with limited resources, and I wouldn\u0026rsquo;t be the person I am today without access to content like this.\n","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]